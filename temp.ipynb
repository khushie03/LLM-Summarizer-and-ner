{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer      \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = model(input_ids)[0]\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    preds = [tags.names[p] for p in predictions[0].device().numpy()]\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import pandas as pd\n",
    "text_de = \"Google Office is in London\"\n",
    "tags = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Tokens   Tags\n",
      "0  ▁Google  B-PER\n",
      "1  ▁Office  B-PER\n",
      "2      ▁is  B-PER\n",
      "3      ▁in  B-PER\n",
      "4  ▁London  B-PER\n",
      "5     None  B-PER\n",
      "6     None  B-PER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Sample text and tags\n",
    "text_de = \"Google Office is in London\"\n",
    "tags = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-base\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "def tag_text(text, tags, model, tokenizer):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = model(input_ids).logits\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    preds = [tags[p] for p in predictions[0].cpu().numpy()]\n",
    "    \n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"]).T\n",
    "\n",
    "df = tag_text(text_de, tags, model, tokenizer)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0        1        2    3    4        5     6\n",
      "Tokens  <s>  ▁Google  ▁Office  ▁is  ▁in  ▁London  </s>\n",
      "Tags      O    B-ORG    I-ORG    O    O    B-LOC     O\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "text_de = \"Google Office is in London\"\n",
    "tags = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Khushiee/xlm-roberta-base-finetuned-panx-ner-1\").to(device)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "def tag_text(text, tags, model, tokenizer):\n",
    "    tokens = tokenizer(text).tokens()  \n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device) \n",
    "    outputs = model(input_ids)[0] \n",
    "    predictions = torch.argmax(outputs, dim=2) \n",
    "    preds = [tags[p] for p in predictions[0].cpu().numpy()] \n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])\n",
    "\n",
    "df = tag_text(text_de, tags, model, xlmr_tokenizer)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n",
      "Sam recommends the new AI tool to Alex and Jordan. The tool can handle complex documents without missing any key points. Alex might use it for his thesis.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "def summarize_dialogue(custom_dialogue):\n",
    "    try:\n",
    "        model_name = \"Khushiee/pegasus-samsum-summarization\"\n",
    "        tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "        model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "        inputs = tokenizer(custom_dialogue, return_tensors=\"pt\", truncation=True)\n",
    "        summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with the custom model: {e}\")\n",
    "        return None\n",
    "\n",
    "custom_dialogue = \"\"\"Sam: Hey Alex, have you tried the new AI tool everyone's talking about?\n",
    "Alex: Oh, you mean the one that helps you summarize documents in seconds?\n",
    "Sam: Yeah, that's the one! I just used it for a project, and it saved me hours of work.\n",
    "Jordan: I’ve been meaning to check it out. Does it handle complex documents well?\n",
    "Sam: Surprisingly, yes. It even managed to summarize a 50-page research paper without missing any key points.\n",
    "Alex: That’s impressive! I might use it for my thesis. Do you think it could handle technical terms?\n",
    "Sam: Definitely. You just have to tweak the input a bit if it's too jargon-heavy, but it works like a charm.\n",
    "Jordan: Alright, I’m sold. Let’s see if this AI tool can make my life a little easier.\n",
    "Sam: Trust me, it will!\n",
    "\"\"\"\n",
    "\n",
    "summary_output = summarize_dialogue(custom_dialogue)\n",
    "if summary_output:\n",
    "    print(\"Model Summary:\")\n",
    "    print(summary_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'AutoModelForSeq2SeqLM' has no attribute 'clear_cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM\n\u001b[1;32m----> 3\u001b[0m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclear_cache\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'AutoModelForSeq2SeqLM' has no attribute 'clear_cache'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "AutoModelForSeq2SeqLM.clear_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
